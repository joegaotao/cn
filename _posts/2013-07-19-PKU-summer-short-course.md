---
date: '2013-07-19'
layout: post
title: PKU暑期高维统计学习心得(I)
categories:
- 感想
- 统计理论
tags:
- PKU
- 统计
- 高维
- 老师
---

### 印象

为其两个周的北大关于高维统计的暑期课程即将告一段落，我回来奔跑了两周，身体略感疲惫，现在总算可以休息下，然后停下来消化下讲过的内容。

这次来讲课的老师学术能力都很强，都是四大paper等身的青年学者。老师们讲课的风格不一，最好玩的当属Tiefeng Jiang老师，他讲起课来就像说东北二人转，段子一个接一个，东北味的口音让我第一节课毫不犯困。而且深入浅出，随机矩阵这种比较数学的研究领域，也被他讲的比较好理解。不过后面由于有事情，以及之后的内容过于数学化，我就没有再跟下去了。Zhu Ji老师讲的很细致，不过内容偏简单了，听了两节课后我也没有跟下去。Cun-Hui Zhang老师做的很理论，深厚的数理分析功底，以及对高维问题理解的深刻让我感觉很敬畏，不敢靠近。对他后面做的scaled lasso和LPDE的结果很感兴趣，想用来做点检验的试验，不过邮件找老师要代码现在还没有回复，略感伤心，看来只能过几天自己写了。Yang Feng老师很年轻，在Fan老师那边做了很多非常好的工作，不过由于之前我看了不少Fan老师的东西，对他的讲的思路相对比较熟悉，也就没有太用心听而刷微博、做项目去了，真是一大罪过啊！

整个课程中对统计所持的观点和态度，我最欣赏的是Hui Zou和Jiashun Jin老师。

<!-- more -->

Hui Zou老师在变量选择、图模型做了不少很好的工作，比如现在很常用的elestic net、adaptive lasso等，都是非常简约而好用的工具。Hui Zou老师为人谦虚，对自己所做的东西不夸耀、不吹捧，他认为统计的工作更像是“完成一个产品”的工作，做出来的方法最好能做成软件包为人所用，而且还要比较好用，所以他的文章不少都会附上R包。这一点我很喜欢，统计本身就是一个应用的学科，如果做的过于数理，缺少实际的价值，并且算法写的没效率没法用，这些都是没法促成统计在现实生活大规模应用的。我觉得当前统计之所以这么热，也主要是当年统计从英国转入美国后，有了Tukey等人不断地大力推动数据分析的理念，推进一些有效的统计分析方法，才有了现在统计一片大热的局面和现在所谓大数据的时代。

Zou Hui老师还提倡多做实验，多种方法多做比较，不要限制于一种方法上。我深以为然。以前我学习统计的感觉就是一定要找一个方法完美的解决这个问题，和做数学问题样，做到一个唯一解。后面我逐渐的体悟到，统计面对的是数据，它本身就是具有随机性的，用多种方法来看这个数据虽然结果会有差异，也许某个方法表现比较好，但是不是说明这个方法在后面遇到了同类型的问题时候，在使用这个方法的效果就一定会好。就拿各种penalty的方法，真实数据你也不知道信噪比如何，回归系数是怎么样，也许模拟结果显示某某方法很好，超越了其他方法，但是面对真实数据，好的方法只是“概率性”地增加了我的信心，我无法确定scad一定比lasso分析的好，何况那些oracle性质只是概率意义上的呢，谁知道不会发生小概率事件并且后面Jiashun老师提到的rare/weak signal问题更加增加了我对这些方法的恐慌。所以，做完理论后，回归到数据分析，唯一的办法就是多做比较，大胆假设，小心论证，发现共同的证据，这才是做统计和做数据分析的思维。

整个暑期课程对我思维激发最大的是Jiashun Jin老师的课程内容。由于课程进度有些快，加上这几天比较忙，我也没有研读老师paper，所以此处只是记录些大概想法，后面有时间会深入探讨。

### Higher Criticism and Rare/Weak Signals 

Jiashun老师讲关于稀疏、弱信号（rare/weak signals）共三节课，最核心的是Higher Criticism and rare/weak signals，然后还有就是关于变量选择的新思维。


关于稀疏、弱信号，Jiashun老师认为在大p小n的情况下，有许多没有用的特征，当真实信号非常稀疏和微弱时，参数空间存在着一块**不可能对参数进行很好推断**的区域。

而导致信号过弱的情况，一个直接原因就是样本过少。信号强度以样本量存在一个2次的比例关系（一般CLT的速度）

$$\text{(signal strength)^2} \varpropto n \varpropto \text{dollars or manpower}$$

这是一个很浅显的道理，增加样本（如果样本不是高度相关抽样所得），信号肯定会增强，但是很多情况下，随着样本增加，成本会大大提高，或者是维数又会大大增加，信号仍旧比较弱，那么此时如何去恢复或者估计呢？

很多情况下，人们都认为他们的数据中信号是很强的，所以可以直接用那些高维的惩罚方法来恢复信号，或者认为强信号与弱信号之间存在巨大的鸿沟，他们可能没法互相转化，又或者认为信号很弱时，我们什么都不用干，因为什么方法都没用。一般来说，大海里捞针，信号本身确实挺弱的，要想寻找到这样的信号，确实是件非常难的事情。但是我们可以提出一个问题：什么样的情况下我们可以通过一些高维的方法找到这样的弱信号，在什么样的情况下我们又无法很好找到弱信号呢？如何量化这种信号可估和不可估的区域呢？

Jiashun老师从FDR的弱点出发引出了自己的思路。

对于简单的问题

$$Y_i = \mu_i + \sigma z_i, \quad z_i \overset{iid}{\sim} N(0, 1), \quad i = 1, 2, \ldots, p$$

如果只有很少量的信号$\mu_i$不为0，挑选信号的一个直接的方法就是用Wavelet hard-thresholding，给出一个阈值

$$
\hat{\mu}_i^H = \left \{
\begin{array}{lc}
y_i, & |y_i| \geq \sigma \cdot t \\
0, & \text{otherwise}
\end{array}
\right.
$$

这个时候选择阈值 $t$ 就是一个艺术化的工作，选大了会导致很多信号选不到，选小了就会导致很多噪音进来。一种选择阈值的方法即通过控制FDR水平（错误发现率），通俗的说，如果能使得选出来的信号中是假信号（噪音）的比例控制在一定水平之下，这样我们也是可以接受的，毕竟真信号还是选出来了，只是附带了一部分噪音罢了。想法是好的，但是实际中，用FDR控制阈值很可能选不到任何信号，因为我们期望FDR能有效果是基于一个信念：信号虽然稀疏，但是还是**强（strong）**的，所以我们也许还是可以找到个相对好的阈值 $t$ 来找到强信号。但是现实中如果信号是弱的，信噪比比较高时，用FDR报告出来的信号便很可能是假的，因此控制FDR还是无法到达选较好阈值的目的。按Jiashun老师的话说，FDR其实与阈值选择没有太大关系，两个不太一样的目标。

于是Jiashun老师从检测稀疏混合分布（Detection of sparse mixtures）出来来导出他的想法和框架，与FDR有些类似，但是效果却大不相同。

做如下假设检验：

$$
\begin{array}{lr}
H_0 : X_1 \overset{iid}{\sim}N(0, 1), & 1 \leq i \leq p \\
H_1^{(p)}: X_i \overset{iid}{\sim}(1 - \epsilon_p)N(0, 1) + \epsilon N(\tau_p, 1), & 1 \leq i \leq p
\end{array}
$$

原假设即各变量是噪音，备择假设是各变量是一种噪音与信号的混合。其中参数有如下形式

$$
\begin{array}{lc}
\epsilon_p = p^{-\beta}, & 0.5 < \beta < 1 \\
\tau_p = \sqrt{2r\log p}, & 0 < r < 1
\end{array}
$$

* 当 $\epsilon_p$ 很小时，比如 $\epsilon_p \ll 1/\sqrt{p}$ 时，意味着只有极少的非零均值，此参数刻画着信号稀疏性（ $\beta$ 越大，$\tau_p$ 越小，信号越越稀疏）；
* 当 $\tau_p$ 比较小时，信号相对比较弱，此参数刻画着信号的强弱（$r$ 越大，信号越强）；一般 $\tau_p < \sqrt{2 \log p}$ 时，信号就凑合能用了（only moderate significance）。

对于两个分布的检验（上述参数固定时候），Neyman-Pearson检验最优。那么自然我们就想通过似然比检验来刻画上述参数($\beta, r$)不同区间的检验效力了。于是就有了如下非常惊艳的有关信号检测的Phase Diagram

![hc](http://i.imgur.com/a0iiwlU.png)

此处划分了四个区域：可精确恢复（exact recovery）；几乎能全恢复（almost full recovery）；可检测的（detectable）；不可检测的（undetectable）。这些都是概率的语言，表示的概率强度不同。横坐标 $\beta$ 越大表示信号越稀疏，纵坐标 $r$ 越大表示信号越强。很多理论的结论都是在 $r > 1$ 时的结论，也就是信号很强的时候咋算都会又不错的估计效果。右图是将横纵坐标都限制在 $(0, 1)$ 区间中，而这一块也正是我们感兴趣的地方，信号稀疏而且很弱的时候估计效果如何？经过一些与检验相关的计算，这些曲线是可以直接算出来的，可以刻画可检测、不可检测、可估计的区域范围。

我觉得这是一个非常能激发思维的结论。对于不可检测的区域，过于稀疏和过于弱的信号，尝试努力恢复的性价比是非常低的，几乎不可能；对于可估（estimatable）的区域，用现在常见的penalty方法基本可以做到比较好的恢复，能够分离开信号与噪音；但是对于可检测（detectable）的区域，虽然我们知道那里面有信号，但是几乎不可能将它们与噪音区分开（FDR失效），不过如果是做信号检测、分类、聚类等工作，进行有效的推断还是仍然有可能的。此时进行推断的框架不是FDR，而需要一个对稀疏、弱信号更敏感的框架，它有个响亮的名字——Higher Criticism。

Higher Criticism，我直译为为高阶鉴别法，Jiashun老师说始于Tukey 1976年Stat 411课程讲义笔记，大师的思维光芒真是能穿越历史呀。Jiashun老师推导的HC与Tukey的略有不同，更为一般化，式子如下：

$$
HC^{*}_p = \max_{0 \leq \alpha \leq \alpha_0}\big\{
\sqrt{p}\big[\frac{\text{fraction significant at }\alpha - \alpha}{\sqrt{\alpha(1 - \alpha)}}\big]
\big\}
$$

其中 $\alpha_0$ 可以是1/2或1。一眼就可以看出来，这是一个比例估计的检验——分子是在控制 $\alpha$ 水平时实际个体显著的比例与真实比例 $\alpha$ 的差异；分母将 $\sqrt{p}$ 拿到分母下就是比例 $\alpha$ 的方差。那么这么做的含义是什么？

仔细想下，这蕴含着一个二阶显著检验问题（second-level significance testing）。想要知道在哪个水平下，我们检验的显著个体是真实，如果只看在某个水平下是否显著（一阶证据），然后依此证据来寻找显著个体其实并不十分理智。比如做了250次独立的检验，有11个在5%水平下显著，实际期望的平均显著个数是250*0.05 = 12.5个，也就是说在原假设为真的情况——假信号（噪音），也会有12.5个会显著。而11个与12.5个有差距很小的，所以我们很有理由怀疑这11个显著的信号不是是真信号，而很可能都是噪音。如果实际显著个数比期望显著个数大很多，那么我们可能更愿意相信在该显著水平 $\alpha$ 下，真能会发现不少的信号。所以我们的目标就是想要调 $\alpha$，看哪个水平下，这个HC值最大，这时候我们可以认为在这个水平下，我们可以发现信号，是可以检测的。

Jiashun老师说，HC值对强信号、弱信号检测都非常敏感，而FDR仅对强信号敏感。我粗浅地想可能就是HC值基于p值后又做了一次检验的缘故吧。由于没有去做Jiashun老师留下的作业，所以理解还不深刻。后面还是回头再算算Phase Diagram中的边界曲线来加深理解。

Higher Cirticism实施比较简单，过程与FDR过程很类似。步骤如下：

1. 对每个特征都算一个z-score，然后根据z-score算个p值，
2. 对p值排序：

$$\pi_{(1)} < \pi_{(2)} < \cdots < \pi_{(p)}$$

3. 计算第$k$个HC值，也相当于算了一个z-score：

$$HC_{p, k} = \sqrt{p}\big[\frac{\frac{k}{p}-\pi_{(k)}}{\sqrt{\pi_{(k)}(1 - \pi_{k})}}\big]$$

4. 取最大值，计算相应的 $HC^{*}_p$ 值，找到对应的 $k$，前 $k$ 可以认为是真显著的。

$$HC^{*}_p = \max_{1 \leq i \leq \alpha_0\cdot p}\{HC_{p,i}\}$$

对应着下面的图形大约可以可以理解这个过程，横轴是实际比例 $k/p$，目的就是找到一个阈值，可以帮助我们检测到信号。

![hc1](http://i.imgur.com/kGMsmoB.png)


然后Jiashun老师给出了他在2004年和他导师Donoho的一篇论文的结果，证明了 $HC^{\ast}_p$ 有最优的适应性(adaptivity)，证明 $HC^{\ast}_p > \sqrt{4\log\log(p)}$ 时，可以获得犯第一类错误与第二类错误之和趋近于0。

Higher Criticism在宇宙学、天文学、基因、异常检测中研究比较多，因为那里的信号比较稀疏和弱，常规的方法已经不能满足需求。另外，HC非常适用于高维的screening、signal detection、classification、clustering等方向，用HC来控制screening中的阈值，比常规的CV、FDR等方法提供了一个新的角度，并且简单有效，无需调参，理论性质也挺好。

P.s. 一不小心突然发现写太多了，本只是记录下心得，不过写着写着觉得还是要重新捋一捋思路才行。之所以写这么多，很大原因是我对penalty太细节化的讨论感到有些厌倦了，里面谈到的统计思想性的东西并不多，所以Jiashun老师东西对我来说比较新颖，便一下子记录了不少，以留作后续继续研读。

Jiashun老师后面还回顾了L0方法的本源，然后说明了在在稀疏、弱信号下，基于L0而衍生的一系列penalty方法都存在比较大的问题。这个论证让我感觉耳目一新，留作下篇再续。希望后面能在深入了解下Jiashun老师的工作，能够有更深的理解，能跳出当前的状态，既能看到他的方法好处，也能看到他的方法的弱点所在，因为我相信没有一种方法是万能的，总会有不完美的地方。

### 送别

总之，两个星期的课程悄然结束。最后Yang Feng老师说希望大家有一个欢乐的暑假时，我才意识到课程真的已经结束。这也意味着陪同我一起上课好朋友兼极客同志——小南，晓矛师弟、赛姐师妹即将离开北京，各自踏上自己的征途。回家写R包的写R包，远赴米国读博的读博，而我，还要坚守在北京，继续着前进的道路。其实感慨良多，因为研一这一年经历了不少心理的改变，尤其是2013年。不过，无论做什么，就全力以赴吧。遥祝晓矛、赛姐米国修炼过程顺利顺心，早日学成归来。祝小南同志潜心修道，将学术理论进行到底，早日成为一名极客+理论家。

[本次暑假课程的PPT（最后一天的还没有）加我下载的相关的论文在此了](http://pan.baidu.com/share/link?shareid=2717730273&uk=771858297)，愿喜欢这块内容的诸君好好学习！